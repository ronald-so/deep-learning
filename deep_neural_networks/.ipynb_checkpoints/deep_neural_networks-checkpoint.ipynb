{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Composed of multiple hidden layers\n",
    "- Allows the functions to be learned that shallower neural networks cannot\n",
    "\n",
    "### Notation\n",
    "\n",
    "- $L$ denotes number of layers (excluding input layer, but including output)\n",
    "- $n^{[l]}$ denotes the number of units in layer $l$\n",
    "    - Input layer is layer 0\n",
    "    \n",
    "    \n",
    "### Forward Propagation\n",
    "\n",
    "- For layer 1:\n",
    "    - $z^{[1]} = W^{[1]}x + b^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$\n",
    "    - $a^{[1]} = g^{[1]}(z^{[1]})$\n",
    "- For layer 2:\n",
    "    - $z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$\n",
    "    - $a^{[2]} = g^{[2]}(z^{[2]})$\n",
    "- and so forth, where $W$ is the weight matrix, $b$ is the bias vector.\n",
    "- Input $a^{[l-1]}$, output $a^{[l]}$ and $z^{[l]}$ (cache)\n",
    "\n",
    "\n",
    "#### Vectorized Implementation\n",
    "\n",
    "- In general:\n",
    "    - $Z^{[l]} = W^{[l]} A{[l-1]} + B{[l]}$\n",
    "    - $A^{[l]} = g^{[l]} (Z^{[l]})$\n",
    "    \n",
    "    \n",
    "### Backward Propagation\n",
    "\n",
    "- Input $da^{[l]}$, outputs $da^{[l-1]}, dW^{[l]}, db^{[l]}$\n",
    "    - $dz^{[l]} = da^{[l]} \\cdot g^{[l]'} (z^{[l]})$\n",
    "    - $dW^{[l]} = dz^{[l]} \\cdot a^{[l-1]}$\n",
    "    - $db^{[l]} = dz^{[l]}$\n",
    "    - $da^{[l-1]} = W^{[l]^{T}} \\cdot dz^{[l]}$\n",
    "\n",
    "#### Vectorized Implementation\n",
    "\n",
    "- In general:\n",
    "    - $dZ^{[l]} = dA^{[l]} \\cdot g^{[l]'} (Z^{[l]})$\n",
    "    - $dW^{[l]} = \\dfrac{1}{m} dZ^{[l]} \\cdot A^{[l-1]T}$\n",
    "    - $dB^{[l]} = \\dfrac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True)$\n",
    "    \n",
    "    - $dA^{[l-1]} = W^{[l]T} \\cdot dZ^{[l]}$\n",
    "    \n",
    "    \n",
    "### Hyperparameters\n",
    "\n",
    "- Parameters are $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, W^{[3]}, b^{[3]}, \\cdots$\n",
    "- Hyperparameters are values which determine the parameters $W$ and $b$, such as:\n",
    "    - Learning rate, $\\alpha$\n",
    "    - Number of iterations\n",
    "    - Number of hidden layers $L$\n",
    "    - Number of hidden units $n^{[1]}, n^{[2]}$, etc\n",
    "    - Choice of activation function (ReLU, sigmoid, tanh)\n",
    "    \n",
    "- Deep learning is a very empirical process, a lot of trial and error is needed to create a good model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitcbaf11bcc615474d8de00896d066064a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
